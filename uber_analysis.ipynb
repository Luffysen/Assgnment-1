{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84da78cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment setup complete!\n"
     ]
    }
   ],
   "source": [
    "# Import essential libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options for better data viewing\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b49b549c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATASET BASIC INFORMATION ===\n",
      "Dataset shape: (200000, 9)\n",
      "Number of rows: 200,000\n",
      "Number of columns: 9\n",
      "\n",
      "=== FIRST 5 ROWS ===\n",
      "   Unnamed: 0                            key  fare_amount  \\\n",
      "0    24238194    2015-05-07 19:52:06.0000003          7.5   \n",
      "1    27835199    2009-07-17 20:04:56.0000002          7.7   \n",
      "2    44984355   2009-08-24 21:45:00.00000061         12.9   \n",
      "3    25894730    2009-06-26 08:22:21.0000001          5.3   \n",
      "4    17610152  2014-08-28 17:47:00.000000188         16.0   \n",
      "\n",
      "           pickup_datetime  pickup_longitude  pickup_latitude  \\\n",
      "0  2015-05-07 19:52:06 UTC        -73.999817        40.738354   \n",
      "1  2009-07-17 20:04:56 UTC        -73.994355        40.728225   \n",
      "2  2009-08-24 21:45:00 UTC        -74.005043        40.740770   \n",
      "3  2009-06-26 08:22:21 UTC        -73.976124        40.790844   \n",
      "4  2014-08-28 17:47:00 UTC        -73.925023        40.744085   \n",
      "\n",
      "   dropoff_longitude  dropoff_latitude  passenger_count  \n",
      "0         -73.999512         40.723217                1  \n",
      "1         -73.994710         40.750325                1  \n",
      "2         -73.962565         40.772647                1  \n",
      "3         -73.965316         40.803349                3  \n",
      "4         -73.973082         40.761247                5  \n",
      "\n",
      "=== LAST 5 ROWS ===\n",
      "        Unnamed: 0                           key  fare_amount  \\\n",
      "199995    42598914  2012-10-28 10:49:00.00000053          3.0   \n",
      "199996    16382965   2014-03-14 01:09:00.0000008          7.5   \n",
      "199997    27804658  2009-06-29 00:42:00.00000078         30.9   \n",
      "199998    20259894   2015-05-20 14:56:25.0000004         14.5   \n",
      "199999    11951496  2010-05-15 04:08:00.00000076         14.1   \n",
      "\n",
      "                pickup_datetime  pickup_longitude  pickup_latitude  \\\n",
      "199995  2012-10-28 10:49:00 UTC        -73.987042        40.739367   \n",
      "199996  2014-03-14 01:09:00 UTC        -73.984722        40.736837   \n",
      "199997  2009-06-29 00:42:00 UTC        -73.986017        40.756487   \n",
      "199998  2015-05-20 14:56:25 UTC        -73.997124        40.725452   \n",
      "199999  2010-05-15 04:08:00 UTC        -73.984395        40.720077   \n",
      "\n",
      "        dropoff_longitude  dropoff_latitude  passenger_count  \n",
      "199995         -73.986525         40.740297                1  \n",
      "199996         -74.006672         40.739620                1  \n",
      "199997         -73.858957         40.692588                2  \n",
      "199998         -73.983215         40.695415                1  \n",
      "199999         -73.985508         40.768793                1  \n",
      "\n",
      "=== COLUMN INFORMATION ===\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 200000 entries, 0 to 199999\n",
      "Data columns (total 9 columns):\n",
      " #   Column             Non-Null Count   Dtype  \n",
      "---  ------             --------------   -----  \n",
      " 0   Unnamed: 0         200000 non-null  int64  \n",
      " 1   key                200000 non-null  object \n",
      " 2   fare_amount        200000 non-null  float64\n",
      " 3   pickup_datetime    200000 non-null  object \n",
      " 4   pickup_longitude   200000 non-null  float64\n",
      " 5   pickup_latitude    200000 non-null  float64\n",
      " 6   dropoff_longitude  199999 non-null  float64\n",
      " 7   dropoff_latitude   199999 non-null  float64\n",
      " 8   passenger_count    200000 non-null  int64  \n",
      "dtypes: float64(5), int64(2), object(2)\n",
      "memory usage: 13.7+ MB\n",
      "None\n",
      "\n",
      "=== DATA TYPES ===\n",
      "Unnamed: 0             int64\n",
      "key                   object\n",
      "fare_amount          float64\n",
      "pickup_datetime       object\n",
      "pickup_longitude     float64\n",
      "pickup_latitude      float64\n",
      "dropoff_longitude    float64\n",
      "dropoff_latitude     float64\n",
      "passenger_count        int64\n",
      "dtype: object\n",
      "\n",
      "=== COLUMN NAMES ===\n",
      "['Unnamed: 0', 'key', 'fare_amount', 'pickup_datetime', 'pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', 'passenger_count']\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('uber.csv')\n",
    "\n",
    "print(\"=== DATASET BASIC INFORMATION ===\")\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Number of rows: {df.shape[0]:,}\")\n",
    "print(f\"Number of columns: {df.shape[1]}\")\n",
    "\n",
    "print(\"\\n=== FIRST 5 ROWS ===\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\n=== LAST 5 ROWS ===\")\n",
    "print(df.tail())\n",
    "\n",
    "print(\"\\n=== COLUMN INFORMATION ===\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\n=== DATA TYPES ===\")\n",
    "print(df.dtypes)\n",
    "\n",
    "print(\"\\n=== COLUMN NAMES ===\")\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d6e2121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATA QUALITY ASSESSMENT ===\n",
      "\n",
      "--- Missing Values Analysis ---\n",
      "              Column  Missing_Count  Missing_Percentage\n",
      "6  dropoff_longitude              1              0.0005\n",
      "7   dropoff_latitude              1              0.0005\n",
      "\n",
      "--- Duplicate Analysis ---\n",
      "Number of duplicate rows: 0\n",
      "Percentage of duplicates: 0.00%\n",
      "Number of duplicate keys: 0\n",
      "\n",
      "--- Statistical Summary ---\n",
      "         Unnamed: 0    fare_amount  pickup_longitude  pickup_latitude  \\\n",
      "count  2.000000e+05  200000.000000     200000.000000    200000.000000   \n",
      "mean   2.771250e+07      11.359955        -72.527638        39.935885   \n",
      "std    1.601382e+07       9.901776         11.437787         7.720539   \n",
      "min    1.000000e+00     -52.000000      -1340.648410       -74.015515   \n",
      "25%    1.382535e+07       6.000000        -73.992065        40.734796   \n",
      "50%    2.774550e+07       8.500000        -73.981823        40.752592   \n",
      "75%    4.155530e+07      12.500000        -73.967154        40.767158   \n",
      "max    5.542357e+07     499.000000         57.418457      1644.421482   \n",
      "\n",
      "       dropoff_longitude  dropoff_latitude  passenger_count  \n",
      "count      199999.000000     199999.000000    200000.000000  \n",
      "mean          -72.525292         39.923890         1.684535  \n",
      "std            13.117408          6.794829         1.385997  \n",
      "min         -3356.666300       -881.985513         0.000000  \n",
      "25%           -73.991407         40.733823         1.000000  \n",
      "50%           -73.980093         40.753042         1.000000  \n",
      "75%           -73.963658         40.768001         2.000000  \n",
      "max          1153.572603        872.697628       208.000000  \n",
      "\n",
      "Rows with zero coordinates: 3,968\n",
      "Percentage with zero coordinates: 1.98%\n"
     ]
    }
   ],
   "source": [
    "print(\"=== DATA QUALITY ASSESSMENT ===\")\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\n--- Missing Values Analysis ---\")\n",
    "missing_values = df.isnull().sum()\n",
    "missing_percentage = (missing_values / len(df)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Column': missing_values.index,\n",
    "    'Missing_Count': missing_values.values,\n",
    "    'Missing_Percentage': missing_percentage.values\n",
    "})\n",
    "missing_df = missing_df[missing_df['Missing_Count'] > 0].sort_values('Missing_Count', ascending=False)\n",
    "\n",
    "if len(missing_df) > 0:\n",
    "    print(missing_df)\n",
    "else:\n",
    "    print(\"No missing values found!\")\n",
    "\n",
    "# Check for duplicate rows\n",
    "print(f\"\\n--- Duplicate Analysis ---\")\n",
    "duplicate_count = df.duplicated().sum()\n",
    "print(f\"Number of duplicate rows: {duplicate_count:,}\")\n",
    "print(f\"Percentage of duplicates: {(duplicate_count/len(df)*100):.2f}%\")\n",
    "\n",
    "# Check for duplicate keys (should be unique identifiers)\n",
    "duplicate_keys = df['key'].duplicated().sum()\n",
    "print(f\"Number of duplicate keys: {duplicate_keys:,}\")\n",
    "\n",
    "# Basic statistical summary\n",
    "print(\"\\n--- Statistical Summary ---\")\n",
    "print(df.describe())\n",
    "\n",
    "# Check for zero coordinates (invalid locations)\n",
    "zero_coords = df[(df['pickup_longitude'] == 0) | (df['pickup_latitude'] == 0) |\n",
    "                 (df['dropoff_longitude'] == 0) | (df['dropoff_latitude'] == 0)]\n",
    "print(f\"\\nRows with zero coordinates: {len(zero_coords):,}\")\n",
    "print(f\"Percentage with zero coordinates: {(len(zero_coords)/len(df)*100):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3c4eb97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATA TYPE CONVERSIONS ===\n",
      "Converting pickup_datetime to datetime format...\n",
      "Datetime conversion successful: datetime64[ns, UTC]\n",
      "Date range: 2009-01-01 01:15:22+00:00 to 2015-06-30 23:40:39+00:00\n",
      "\n",
      "--- Passenger Count Analysis ---\n",
      "Unique passenger counts: [0, 1, 2, 3, 4, 5, 6, 208]\n",
      "Passenger count value counts:\n",
      "passenger_count\n",
      "0         709\n",
      "1      138425\n",
      "2       29428\n",
      "3        8881\n",
      "4        4276\n",
      "5       14009\n",
      "6        4271\n",
      "208         1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- Fare Amount Analysis ---\n",
      "Rides with negative or zero fare: 22\n",
      "Percentage: 0.01%\n",
      "Sample of negative/zero fares:\n",
      "                               key  fare_amount           pickup_datetime\n",
      "20744  2015-04-22 23:25:07.0000008          0.0 2015-04-22 23:25:07+00:00\n",
      "22182  2010-03-20 02:59:51.0000002          0.0 2010-03-20 02:59:51+00:00\n",
      "63395  2015-03-03 23:07:41.0000008         -5.0 2015-03-03 23:07:41+00:00\n",
      "71246  2010-02-11 21:47:10.0000001         -3.3 2010-02-11 21:47:10+00:00\n",
      "79903  2015-05-01 14:43:02.0000004         -3.5 2015-05-01 14:43:02+00:00\n"
     ]
    }
   ],
   "source": [
    "print(\"=== DATA TYPE CONVERSIONS ===\")\n",
    "\n",
    "# Convert pickup_datetime to datetime\n",
    "print(\"Converting pickup_datetime to datetime format...\")\n",
    "df[\"pickup_datetime\"] = pd.to_datetime(df[\"pickup_datetime\"])\n",
    "\n",
    "# Verify the conversion\n",
    "print(f\"Datetime conversion successful: {df[\"pickup_datetime\"].dtype}\")\n",
    "print(f\"Date range: {df[\"pickup_datetime\"].min()} to {df[\"pickup_datetime\"].max()}\")\n",
    "\n",
    "# Check for any invalid passenger counts\n",
    "print(f\"\\n--- Passenger Count Analysis ---\")\n",
    "print(f\"Unique passenger counts: {sorted(df[\"passenger_count\"].unique())}\")\n",
    "print(f\"Passenger count value counts:\")\n",
    "print(df[\"passenger_count\"].value_counts().sort_index())\n",
    "\n",
    "# Check for negative or zero fare amounts\n",
    "print(f\"\\n--- Fare Amount Analysis ---\")\n",
    "negative_fares = df[df[\"fare_amount\"] <= 0]\n",
    "print(f\"Rides with negative or zero fare: {len(negative_fares):,}\")\n",
    "print(f\"Percentage: {(len(negative_fares)/len(df)*100):.2f}%\")\n",
    "if len(negative_fares) > 0:\n",
    "    print(\"Sample of negative/zero fares:\")\n",
    "    print(negative_fares[[\"key\", \"fare_amount\", \"pickup_datetime\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ab3f94e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATA CLEANING PROCESS ===\n",
      "Original dataset size: 200,000 rows\n",
      "After removing duplicates: 200,000 rows (0 removed)\n",
      "After removing zero coordinates: 196,032 rows (3,968 removed)\n",
      "After removing invalid fares: 196,013 rows (19 removed)\n",
      "After removing invalid passenger counts: 195,326 rows (687 removed)\n",
      "\n",
      "Fare amount outlier bounds:\n",
      "Lower bound: $-13.50\n",
      "Upper bound: $32.00\n",
      "Extreme fare outliers found: 8,688\n",
      "\n",
      "=== CLEANING SUMMARY ===\n",
      "Original size: 200,000 rows\n",
      "Final size: 195,326 rows\n",
      "Total removed: 4,674 rows (2.34%)\n",
      "Data retention rate: 97.66%\n",
      "\n",
      "=== CLEANED DATA VERIFICATION ===\n",
      "No missing values: True\n",
      "No zero coordinates: True\n",
      "All positive fares: True\n",
      "All positive passenger counts: True\n"
     ]
    }
   ],
   "source": [
    "print(\"=== DATA CLEANING PROCESS ===\")\n",
    "\n",
    "# Store original dataset size\n",
    "original_size = len(df)\n",
    "print(f\"Original dataset size: {original_size:,} rows\")\n",
    "\n",
    "# Step 5.1: Remove duplicate rows\n",
    "df_cleaned = df.drop_duplicates()\n",
    "after_duplicates = len(df_cleaned)\n",
    "print(f\"After removing duplicates: {after_duplicates:,} rows ({original_size - after_duplicates:,} removed)\")\n",
    "\n",
    "# Step 5.2: Remove rows with zero coordinates (invalid locations)\n",
    "df_cleaned = df_cleaned[\n",
    "    (df_cleaned[\"pickup_longitude\"] != 0) &\n",
    "    (df_cleaned[\"pickup_latitude\"] != 0) &\n",
    "    (df_cleaned[\"dropoff_longitude\"] != 0) &\n",
    "    (df_cleaned[\"dropoff_latitude\"] != 0)\n",
    "]\n",
    "after_coordinates = len(df_cleaned)\n",
    "print(f\"After removing zero coordinates: {after_coordinates:,} rows ({after_duplicates - after_coordinates:,} removed)\")\n",
    "\n",
    "# Step 5.3: Remove negative or zero fare amounts\n",
    "df_cleaned = df_cleaned[df_cleaned[\"fare_amount\"] > 0]\n",
    "after_fares = len(df_cleaned)\n",
    "print(f\"After removing invalid fares: {after_fares:,} rows ({after_coordinates - after_fares:,} removed)\")\n",
    "\n",
    "# Step 5.4: Remove invalid passenger counts (0 or negative)\n",
    "df_cleaned = df_cleaned[df_cleaned[\"passenger_count\"] > 0]\n",
    "after_passengers = len(df_cleaned)\n",
    "print(f\"After removing invalid passenger counts: {after_passengers:,} rows ({after_fares - after_passengers:,} removed)\")\n",
    "\n",
    "# Step 5.5: Remove extreme outliers in fare amounts (optional - be careful with this)\n",
    "# Calculate IQR for fare amounts\n",
    "Q1 = df_cleaned[\"fare_amount\"].quantile(0.25)\n",
    "Q3 = df_cleaned[\"fare_amount\"].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 3 * IQR # Using 3*IQR instead of 1.5*IQR for less aggressive outlier removal\n",
    "upper_bound = Q3 + 3 * IQR\n",
    "\n",
    "print(f\"\\nFare amount outlier bounds:\")\n",
    "print(f\"Lower bound: ${lower_bound:.2f}\")\n",
    "print(f\"Upper bound: ${upper_bound:.2f}\")\n",
    "\n",
    "# Count extreme outliers\n",
    "extreme_outliers = df_cleaned[(df_cleaned[\"fare_amount\"] < lower_bound) |\n",
    "                              (df_cleaned[\"fare_amount\"] > upper_bound)]\n",
    "print(f\"Extreme fare outliers found: {len(extreme_outliers):,}\")\n",
    "\n",
    "# Optionally remove extreme outliers (uncomment if needed)\n",
    "# df_cleaned = df_cleaned[(df_cleaned[\"fare_amount\"] >= lower_bound) &\n",
    "#                         (df_cleaned[\"fare_amount\"] <= upper_bound)]\n",
    "# after_outliers = len(df_cleaned)\n",
    "# print(f\"After removing extreme outliers: {after_outliers:,} rows\")\n",
    "\n",
    "# Final cleaning summary\n",
    "\n",
    "final_size = len(df_cleaned)\n",
    "total_removed = original_size - final_size\n",
    "removal_percentage = (total_removed / original_size) * 100\n",
    "\n",
    "print(f\"\\n=== CLEANING SUMMARY ===\")\n",
    "print(f\"Original size: {original_size:,} rows\")\n",
    "print(f\"Final size: {final_size:,} rows\")\n",
    "print(f\"Total removed: {total_removed:,} rows ({removal_percentage:.2f}%)\")\n",
    "print(f\"Data retention rate: {((final_size/original_size)*100):.2f}%\")\n",
    "\n",
    "# Verify cleaned data\n",
    "print(f\"\\n=== CLEANED DATA VERIFICATION ===\")\n",
    "print(f\"No missing values: {df_cleaned.isnull().sum().sum() == 0}\")\n",
    "print(f\"No zero coordinates: {((df_cleaned[[\"pickup_longitude\",\n",
    "\"pickup_latitude\", \"dropoff_longitude\", \"dropoff_latitude\"]] == 0).sum().sum()\n",
    "== 0)}\")\n",
    "print(f\"All positive fares: {(df_cleaned[\"fare_amount\"] > 0).all()}\")\n",
    "print(f\"All positive passenger counts: {(df_cleaned[\"passenger_count\"] >\n",
    "0).all()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b6f67b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaned dataset saved as: uber_cleaned.csv\n",
      "File size: 195,326 rows × 9 columns\n",
      "\n",
      "=== FINAL CLEANED DATASET INFO ===\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 195326 entries, 0 to 199999\n",
      "Data columns (total 9 columns):\n",
      " #   Column             Non-Null Count   Dtype              \n",
      "---  ------             --------------   -----              \n",
      " 0   Unnamed: 0         195326 non-null  int64              \n",
      " 1   key                195326 non-null  object             \n",
      " 2   fare_amount        195326 non-null  float64            \n",
      " 3   pickup_datetime    195326 non-null  datetime64[ns, UTC]\n",
      " 4   pickup_longitude   195326 non-null  float64            \n",
      " 5   pickup_latitude    195326 non-null  float64            \n",
      " 6   dropoff_longitude  195326 non-null  float64            \n",
      " 7   dropoff_latitude   195326 non-null  float64            \n",
      " 8   passenger_count    195326 non-null  int64              \n",
      "dtypes: datetime64[ns, UTC](1), float64(5), int64(2), object(1)\n",
      "memory usage: 14.9+ MB\n",
      "None\n",
      "\n",
      "First 5 rows of cleaned data:\n",
      "   Unnamed: 0                            key  fare_amount  \\\n",
      "0    24238194    2015-05-07 19:52:06.0000003          7.5   \n",
      "1    27835199    2009-07-17 20:04:56.0000002          7.7   \n",
      "2    44984355   2009-08-24 21:45:00.00000061         12.9   \n",
      "3    25894730    2009-06-26 08:22:21.0000001          5.3   \n",
      "4    17610152  2014-08-28 17:47:00.000000188         16.0   \n",
      "\n",
      "            pickup_datetime  pickup_longitude  pickup_latitude  \\\n",
      "0 2015-05-07 19:52:06+00:00        -73.999817        40.738354   \n",
      "1 2009-07-17 20:04:56+00:00        -73.994355        40.728225   \n",
      "2 2009-08-24 21:45:00+00:00        -74.005043        40.740770   \n",
      "3 2009-06-26 08:22:21+00:00        -73.976124        40.790844   \n",
      "4 2014-08-28 17:47:00+00:00        -73.925023        40.744085   \n",
      "\n",
      "   dropoff_longitude  dropoff_latitude  passenger_count  \n",
      "0         -73.999512         40.723217                1  \n",
      "1         -73.994710         40.750325                1  \n",
      "2         -73.962565         40.772647                1  \n",
      "3         -73.965316         40.803349                3  \n",
      "4         -73.973082         40.761247                5  \n"
     ]
    }
   ],
   "source": [
    "# 6 Save cleaned dataset\n",
    "output_filename = \"uber_cleaned.csv\"\n",
    "df_cleaned.to_csv(output_filename, index=False)\n",
    "\n",
    "print(f\"\\nCleaned dataset saved as: {output_filename}\")\n",
    "print(f\"File size: {len(df_cleaned):,} rows × {len(df_cleaned.columns)} columns\")\n",
    "\n",
    "# Display final dataset info\n",
    "print(\"\\n=== FINAL CLEANED DATASET INFO ===\")\n",
    "print(df_cleaned.info())\n",
    "print(\"\\nFirst 5 rows of cleaned data:\")\n",
    "print(df_cleaned.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a836c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== COMPREHENSIVE DESCRIPTIVE STATISTICS ===\")\n",
    "\n",
    "# Basic descriptive statistics for numerical columns\n",
    "numerical_cols = [\"fare_amount\", \"pickup_longitude\", \"pickup_latitude\",\n",
    "                  \"dropoff_longitude\", \"dropoff_latitude\", \"passenger_count\"]\n",
    "print(\"--- Basic Statistical Measures ---\")\n",
    "desc_stats = df_cleaned[numerical_cols].describe()\n",
    "print(desc_stats)\n",
    "\n",
    "# Additional statistical measures\n",
    "print(\"\\n--- Additional Statistical Measures ---\")\n",
    "for col in numerical_cols:\n",
    "    data = df_cleaned[col]\n",
    "    print(f\"\\n{col.upper()}:\")\n",
    "    print(f\" Mean: {data.mean():.4f}\")\n",
    "    print(f\" Median: {data.median():.4f}\")\n",
    "    print(f\" Mode: {data.mode().iloc[0]:.4f}\")\n",
    "    print(f\" Standard Deviation: {data.std():.4f}\")\n",
    "    print(f\" Variance: {data.var():.4f}\")\n",
    "    print(f\" Skewness: {data.skew():.4f}\")\n",
    "    print(f\" Kurtosis: {data.kurtosis():.4f}\")\n",
    "    print(f\" Range: {data.max() - data.min():.4f}\")\n",
    "    print(f\" IQR: {data.quantile(0.75) - data.quantile(0.25):.4f}\")\n",
    "\n",
    "# Quartile analysis\n",
    "print(\"\\n--- Quartile Analysis ---\")\n",
    "quartiles = df_cleaned[numerical_cols].quantile([0.25, 0.5, 0.75])\n",
    "print(quartiles)\n",
    "\n",
    "# Percentile analysis for fare amounts\n",
    "print(\"\\n--- Fare Amount Percentile Analysis ---\")\n",
    "percentiles = [1, 5, 10, 25, 50, 75, 90, 95, 99]\n",
    "fare_percentiles = df_cleaned[\"fare_amount\"].quantile([p/100 for p in percentiles])\n",
    "for i, p in enumerate(percentiles):\n",
    "    print(f\"{p}th percentile: ${fare_percentiles.iloc[i]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882e1bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== UNIVARIATE ANALYSIS ===\")\n",
    "\n",
    "# Set up the plotting environment\n",
    "plt.style.use(\"default\")\n",
    "fig = plt.figure(figsize=(20, 15))\n",
    "\n",
    "# 8.1: Fare Amount Distribution\n",
    "plt.subplot(3, 3, 1)\n",
    "plt.hist(df_cleaned[\"fare_amount\"], bins=50, alpha=0.7, color=\"skyblue\",\n",
    "         edgecolor=\"black\")\n",
    "plt.title(\"Distribution of Fare Amounts\", fontsize=14, fontweight=\"bold\")\n",
    "plt.xlabel(\"Fare Amount ($)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "# Add statistics to the plot\n",
    "mean_fare = df_cleaned[\"fare_amount\"].mean()\n",
    "median_fare = df_cleaned[\"fare_amount\"].median()\n",
    "plt.axvline(mean_fare, color=\"red\", linestyle=\"--\", label=f\"Mean:\\n${mean_fare:.2f}\")\n",
    "plt.axvline(median_fare, color=\"green\", linestyle=\"--\", label=f\"Median:\\n${median_fare:.2f}\")\n",
    "plt.legend()\n",
    "\n",
    "# 8.2: Fare Amount Box Plot\n",
    "plt.subplot(3, 3, 2)\n",
    "box_plot = plt.boxplot(df_cleaned[\"fare_amount\"], patch_artist=True)\n",
    "box_plot[\"boxes\"][0].set_facecolor(\"lightblue\")\n",
    "plt.title(\"Fare Amount Box Plot\", fontsize=14, fontweight=\"bold\")\n",
    "plt.ylabel(\"Fare Amount ($)\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 8.3: Passenger Count Distribution\n",
    "plt.subplot(3, 3, 3)\n",
    "passenger_counts = df_cleaned[\"passenger_count\"].value_counts().sort_index()\n",
    "plt.bar(passenger_counts.index, passenger_counts.values, color=\"lightcoral\",\n",
    "        alpha=0.7)\n",
    "plt.title(\"Distribution of Passenger Counts\", fontsize=14, fontweight=\"bold\")\n",
    "plt.xlabel(\"Number of Passengers\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "# Add percentage labels on bars\n",
    "total_rides = len(df_cleaned)\n",
    "for i, v in enumerate(passenger_counts.values):\n",
    "    plt.text(passenger_counts.index[i], v + total_rides*0.01,\n",
    "             f\"{(v/total_rides*100):.1f}%\", ha=\"center\", va=\"bottom\")\n",
    "\n",
    "# 8.4: Pickup Longitude Distribution\n",
    "plt.subplot(3, 3, 4)\n",
    "plt.hist(df_cleaned[\"pickup_longitude\"], bins=50, alpha=0.7,\n",
    "         color=\"lightgreen\", edgecolor=\"black\")\n",
    "plt.title(\"Distribution of Pickup Longitude\", fontsize=14, fontweight=\"bold\")\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 8.5: Pickup Latitude Distribution\n",
    "plt.subplot(3, 3, 5)\n",
    "plt.hist(df_cleaned[\"pickup_latitude\"], bins=50, alpha=0.7, color=\"orange\",\n",
    "         edgecolor=\"black\")\n",
    "plt.title(\"Distribution of Pickup Latitude\", fontsize=14, fontweight=\"bold\")\n",
    "plt.xlabel(\"Latitude\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 8.6: Dropoff Longitude Distribution\n",
    "plt.subplot(3, 3, 6)\n",
    "plt.hist(df_cleaned[\"dropoff_longitude\"], bins=50, alpha=0.7, color=\"purple\",\n",
    "         edgecolor=\"black\")\n",
    "plt.title(\"Distribution of Dropoff Longitude\", fontsize=14, fontweight=\"bold\")\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 8.7: Dropoff Latitude Distribution\n",
    "plt.subplot(3, 3, 7)\n",
    "plt.hist(df_cleaned[\"dropoff_latitude\"], bins=50, alpha=0.7, color=\"brown\",\n",
    "         edgecolor=\"black\")\n",
    "plt.title(\"Distribution of Dropoff Latitude\", fontsize=14, fontweight=\"bold\")\n",
    "plt.xlabel(\"Latitude\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 8.8: Log-transformed Fare Amount (to handle skewness)\n",
    "plt.subplot(3, 3, 8)\n",
    "log_fares = np.log1p(df_cleaned[\"fare_amount\"]) # log1p handles zero values better\n",
    "plt.hist(log_fares, bins=50, alpha=0.7, color=\"pink\", edgecolor=\"black\")\n",
    "plt.title(\"Log-Transformed Fare Amounts\", fontsize=14, fontweight=\"bold\")\n",
    "plt.xlabel(\"Log(Fare Amount + 1)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 8.9: Fare Amount vs Passenger Count (Box Plot)\n",
    "plt.subplot(3, 3, 9)\n",
    "df_cleaned.boxplot(column=\"fare_amount\", by=\"passenger_count\", ax=plt.gca())\n",
    "plt.title(\"Fare Amount by Passenger Count\", fontsize=14, fontweight=\"bold\")\n",
    "plt.xlabel(\"Passenger Count\")\n",
    "plt.ylabel(\"Fare Amount ($)\")\n",
    "plt.suptitle(\"\") # Remove the automatic title\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"univariate_analysis.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics for univariate analysis\n",
    "print(\"\\n--- Univariate Analysis Summary ---\")\n",
    "print(f\"Fare Amount - Mean: ${df_cleaned['fare_amount'].mean():.2f}, Std:\\n${df_cleaned['fare_amount'].std():.2f}\")\n",
    "print(f\"Most common passenger count:\\n{df_cleaned['passenger_count'].mode().iloc[0]} passengers\")\n",
    "print(f\"Longitude range: {df_cleaned['pickup_longitude'].min():.4f} to\\n{df_cleaned['pickup_longitude'].max():.4f}\")\n",
    "print(f\"Latitude range: {df_cleaned['pickup_latitude'].min():.4f} to\\n{df_cleaned['pickup_latitude'].max():.4f}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc810f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== BIVARIATE ANALYSIS ===\")\n",
    "\n",
    "# 9.1: Correlation Matrix\n",
    "print(\"--- Correlation Analysis ---\")\n",
    "correlation_matrix = df_cleaned[numerical_cols].corr()\n",
    "print(\"Correlation Matrix:\")\n",
    "print(correlation_matrix.round(4))\n",
    "\n",
    "# Create correlation heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool)) # Mask upper triangle\n",
    "sns.heatmap(correlation_matrix,\n",
    "            mask=mask,\n",
    "            annot=True,\n",
    "            cmap=\"coolwarm\",\n",
    "            center=0,\n",
    "            square=True,\n",
    "            fmt=\".3f\",\n",
    "            cbar_kws={\"shrink\": .8})\n",
    "plt.title(\"Correlation Matrix Heatmap\", fontsize=16, fontweight=\"bold\", pad=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"correlation_heatmap.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# 9.2: Scatter Plot Analysis\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# Fare vs Pickup Longitude\n",
    "axes[0, 0].scatter(df_cleaned[\"pickup_longitude\"], df_cleaned[\"fare_amount\"],\n",
    "                   alpha=0.5, s=1, color=\"blue\")\n",
    "axes[0, 0].set_title(\"Fare Amount vs Pickup Longitude\")\n",
    "axes[0, 0].set_xlabel(\"Pickup Longitude\")\n",
    "axes[0, 0].set_ylabel(\"Fare Amount ($)\")\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Fare vs Pickup Latitude\n",
    "axes[0, 1].scatter(df_cleaned[\"pickup_latitude\"], df_cleaned[\"fare_amount\"],\n",
    "                   alpha=0.5, s=1, color=\"green\")\n",
    "axes[0, 1].set_title(\"Fare Amount vs Pickup Latitude\")\n",
    "axes[0, 1].set_xlabel(\"Pickup Latitude\")\n",
    "axes[0, 1].set_ylabel(\"Fare Amount ($)\")\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Fare vs Passenger Count\n",
    "axes[0, 2].scatter(df_cleaned[\"passenger_count\"], df_cleaned[\"fare_amount\"],\n",
    "                   alpha=0.5, s=1, color=\"red\")\n",
    "axes[0, 2].set_title(\"Fare Amount vs Passenger Count\")\n",
    "axes[0, 2].set_xlabel(\"Passenger Count\")\n",
    "axes[0, 2].set_ylabel(\"Fare Amount ($)\")\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# Pickup vs Dropoff Longitude\n",
    "axes[1, 0].scatter(df_cleaned[\"pickup_longitude\"],\n",
    "                   df_cleaned[\"dropoff_longitude\"],\n",
    "                   alpha=0.5, s=1, color=\"purple\")\n",
    "axes[1, 0].set_title(\"Pickup vs Dropoff Longitude\")\n",
    "axes[1, 0].set_xlabel(\"Pickup Longitude\")\n",
    "axes[1, 0].set_ylabel(\"Dropoff Longitude\")\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Pickup vs Dropoff Latitude\n",
    "axes[1, 1].scatter(df_cleaned[\"pickup_latitude\"],\n",
    "                   df_cleaned[\"dropoff_latitude\"],\n",
    "                   alpha=0.5, s=1, color=\"orange\")\n",
    "axes[1, 1].set_title(\"Pickup vs Dropoff Latitude\")\n",
    "axes[1, 1].set_xlabel(\"Pickup Latitude\")\n",
    "axes[1, 1].set_ylabel(\"Dropoff Latitude\")\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Distance vs Fare (we'll calculate distance first)\n",
    "# Calculate Euclidean distance (simplified)\n",
    "df_cleaned[\"distance\"] = np.sqrt(\n",
    "    (df_cleaned[\"dropoff_longitude\"] - df_cleaned[\"pickup_longitude\"])**2 +\n",
    "    (df_cleaned[\"dropoff_latitude\"] - df_cleaned[\"pickup_latitude\"])**2\n",
    ")\n",
    "axes[1, 2].scatter(df_cleaned[\"distance\"], df_cleaned[\"fare_amount\"],\n",
    "                   alpha=0.5, s=1, color=\"brown\")\n",
    "axes[1, 2].set_title(\"Distance vs Fare Amount\")\n",
    "axes[1, 2].set_xlabel(\"Distance (Euclidean)\")\n",
    "axes[1, 2].set_ylabel(\"Fare Amount ($)\")\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"bivariate_analysis.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# 9.3: Statistical Relationship Analysis\n",
    "print(\"\\n--- Statistical Relationships ---\")\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "# Calculate correlations with p-values\n",
    "relationships = [\n",
    "    (\"fare_amount\", \"distance\"),\n",
    "    (\"fare_amount\", \"passenger_count\"),\n",
    "    (\"pickup_longitude\", \"dropoff_longitude\"),\n",
    "    (\"pickup_latitude\", \"dropoff_latitude\")\n",
    "]\n",
    "\n",
    "for var1, var2 in relationships:\n",
    "    pearson_corr, pearson_p = pearsonr(df_cleaned[var1], df_cleaned[var2])\n",
    "    spearman_corr, spearman_p = spearmanr(df_cleaned[var1], df_cleaned[var2])\n",
    "    print(f\"\\n{var1.upper()} vs {var2.upper()}:\")\n",
    "    print(f\" Pearson correlation: {pearson_corr:.4f} (p-value:\\n{pearson_p:.4e})\")\n",
    "    print(f\" Spearman correlation: {spearman_corr:.4f} (p-value:\\n{spearman_p:.4e})\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0689afcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== OUTLIER DETECTION AND ANALYSIS ===\")\n",
    "\n",
    "# 10.1: IQR Method for Outlier Detection\n",
    "def detect_outliers_iqr(data, column):\n",
    "    Q1 = data[column].quantile(0.25)\n",
    "    Q3 = data[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]\n",
    "    return outliers, lower_bound, upper_bound\n",
    "\n",
    "# 10.2: Z-Score Method for Outlier Detection\n",
    "def detect_outliers_zscore(data, column, threshold=3):\n",
    "    z_scores = np.abs((data[column] - data[column].mean()) / data[column].std())\n",
    "    outliers = data[z_scores > threshold]\n",
    "    return outliers\n",
    "\n",
    "# Analyze outliers for fare amounts\n",
    "print(\"--- Fare Amount Outliers ---\")\n",
    "fare_outliers_iqr, fare_lower, fare_upper = detect_outliers_iqr(df_cleaned, \"fare_amount\")\n",
    "fare_outliers_zscore = detect_outliers_zscore(df_cleaned, \"fare_amount\")\n",
    "\n",
    "print(f\"IQR Method:\")\n",
    "print(f\" Lower bound: ${fare_lower:.2f}\")\n",
    "print(f\" Upper bound: ${fare_upper:.2f}\")\n",
    "print(f\" Number of outliers: {len(fare_outliers_iqr):,} ({len(fare_outliers_iqr)/len(df_cleaned)*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nZ-Score Method (threshold=3):\")\n",
    "print(f\" Number of outliers: {len(fare_outliers_zscore):,} ({len(fare_outliers_zscore)/len(df_cleaned)*100:.2f}%)\")\n",
    "\n",
    "# Visualize outliers\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Box plot with outliers\n",
    "axes[0, 0].boxplot(df_cleaned[\"fare_amount\"], patch_artist=True)\n",
    "axes[0, 0].set_title(\"Fare Amount Box Plot (with outliers)\")\n",
    "axes[0, 0].set_ylabel(\"Fare Amount ($)\")\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Box plot without extreme outliers (for better visualization)\n",
    "fare_no_extreme = df_cleaned[df_cleaned[\"fare_amount\"] <= df_cleaned[\"fare_amount\"].quantile(0.95)]\n",
    "axes[0, 1].boxplot(fare_no_extreme[\"fare_amount\"], patch_artist=True)\n",
    "axes[0, 1].set_title(\"Fare Amount Box Plot (95th percentile cap)\")\n",
    "axes[0, 1].set_ylabel(\"Fare Amount ($)\")\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Histogram with outlier boundaries\n",
    "axes[1, 0].hist(df_cleaned[\"fare_amount\"], bins=100, alpha=0.7, color=\"skyblue\")\n",
    "axes[1, 0].axvline(fare_lower, color=\"red\", linestyle=\"--\", label=f\"Lower\\nbound: ${fare_lower:.2f}\")\n",
    "axes[1, 0].axvline(fare_upper, color=\"red\", linestyle=\"--\", label=f\"Upper\\nbound: ${fare_upper:.2f}\")\n",
    "\n",
    "axes[1, 0].set_title(\"Fare Amount Distribution with IQR Outlier Bounds\")\n",
    "axes[1, 0].set_xlabel(\"Fare Amount ($)\")\n",
    "axes[1, 0].set_ylabel(\"Frequency\")\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Scatter plot: Distance vs Fare with outliers highlighted\n",
    "outlier_indices = fare_outliers_iqr.index\n",
    "normal_data = df_cleaned.drop(outlier_indices)\n",
    "axes[1, 1].scatter(normal_data[\"distance\"], normal_data[\"fare_amount\"], alpha=0.5, s=1, color=\"blue\", label=\"Normal\")\n",
    "axes[1, 1].scatter(fare_outliers_iqr[\"distance\"], fare_outliers_iqr[\"fare_amount\"], alpha=0.7, s=2, color=\"red\", label=\"Outliers\")\n",
    "axes[1, 1].set_title(\"Distance vs Fare (Outliers Highlighted)\")\n",
    "axes[1, 1].set_xlabel(\"Distance\")\n",
    "axes[1, 1].set_ylabel(\"Fare Amount ($)\")\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"outlier_analysis.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# Analyze characteristics of outliers\n",
    "print(f\"\\n--- Outlier Characteristics ---\")\n",
    "if len(fare_outliers_iqr) > 0:\n",
    "    print(\"Top 10 highest fare outliers:\")\n",
    "    top_outliers = fare_outliers_iqr.nlargest(10, \"fare_amount\")[[\"fare_amount\", \"distance\", \"passenger_count\", \"pickup_datetime\"]]\n",
    "    print(top_outliers)\n",
    "    print(f\"\\nOutlier statistics:\")\n",
    "    print(f\" Mean fare of outliers:\\n${fare_outliers_iqr['fare_amount'].mean():.2f}\")\n",
    "    print(f\" Median fare of outliers:\\n${fare_outliers_iqr['fare_amount'].median():.2f}\")\n",
    "    print(f\" Max fare outlier: ${fare_outliers_iqr['fare_amount'].max():.2f}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5bde3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== FEATURE ENGINEERING ===\")\n",
    "\n",
    "# Ensure pickup_datetime is in datetime format\n",
    "df_cleaned[\"pickup_datetime\"] = pd.to_datetime(df_cleaned[\"pickup_datetime\"])\n",
    "\n",
    "# 11.1: Extracting Time-Based Features\n",
    "print(\"Extracting time-based features...\")\n",
    "df_cleaned[\"year\"] = df_cleaned[\"pickup_datetime\"].dt.year\n",
    "df_cleaned[\"month\"] = df_cleaned[\"pickup_datetime\"].dt.month\n",
    "df_cleaned[\"day\"] = df_cleaned[\"pickup_datetime\"].dt.day\n",
    "df_cleaned[\"hour\"] = df_cleaned[\"pickup_datetime\"].dt.hour\n",
    "df_cleaned[\"day_of_week\"] = df_cleaned[\"pickup_datetime\"].dt.dayofweek # Monday=0, Sunday=6\n",
    "df_cleaned[\"day_name\"] = df_cleaned[\"pickup_datetime\"].dt.day_name()\n",
    "df_cleaned[\"month_name\"] = df_cleaned[\"pickup_datetime\"].dt.month_name()\n",
    "\n",
    "# 11.2: Categorizing Day of Week and Time of Day\n",
    "print(\"Categorizing day of week and time of day...\")\n",
    "df_cleaned[\"is_weekend\"] = df_cleaned[\"day_of_week\"].apply(lambda x: 1 if x >= 5 else 0) # Saturday and Sunday\n",
    "\n",
    "def get_time_of_day(hour):\n",
    "    if 7 <= hour <= 9 or 16 <= hour <= 19:\n",
    "        return \"Peak\"\n",
    "    elif 22 <= hour or hour <= 5:\n",
    "        return \"Late Night\"\n",
    "    else:\n",
    "        return \"Off-Peak\"\n",
    "df_cleaned[\"time_of_day\"] = df_cleaned[\"hour\"].apply(get_time_of_day)\n",
    "\n",
    "# 11.3: Calculating Ride Distance (Haversine Distance)\n",
    "print(\"Calculating Haversine distance...\")\n",
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    R = 6371 # Radius of Earth in kilometers\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = np.sin(dlat / 2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    distance = R * c\n",
    "    return distance\n",
    "\n",
    "df_cleaned[\"distance_km\"] = df_cleaned.apply(\n",
    "    lambda row: haversine_distance(\n",
    "        row[\"pickup_latitude\"], row[\"pickup_longitude\"],\n",
    "        row[\"dropoff_latitude\"], row[\"dropoff_longitude\"]\n",
    "    ), axis=1\n",
    ")\n",
    "\n",
    "df_cleaned[\"distance_km\"] = df_cleaned[\"distance_km\"].replace([np.inf, -np.inf], np.nan)\n",
    "df_cleaned[\"distance_km\"].fillna(0, inplace=True) # Replace NaN with 0 for rides with no distance\n",
    "\n",
    "# 11.4: Fare per Kilometer\n",
    "print(\"Calculating fare per kilometer...\")\n",
    "df_cleaned[\"fare_per_km\"] = df_cleaned[\"fare_amount\"] / (df_cleaned[\"distance_km\"] + 0.001) # Add small epsilon to avoid division by zero\n",
    "\n",
    "# Display new features\n",
    "print(\"\\nNew features created:\")\n",
    "print(df_cleaned[[\n",
    "\"pickup_datetime\", \"year\", \"month\", \"day\", \"hour\",\n",
    "\"day_of_week\", \"day_name\", \"month_name\", \"is_weekend\", \"time_of_day\",\n",
    "\"distance_km\", \"fare_per_km\"\n",
    "]].head())\n",
    "\n",
    "print(\"\\nDescriptive statistics for new numerical features:\")\n",
    "print(df_cleaned[[\"distance_km\", \"fare_per_km\"]].describe()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddda8798",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== ENCODING CATEGORICAL VARIABLES ===\")\n",
    "\n",
    "# Identify categorical columns for encoding\n",
    "categorical_cols_to_encode = [\"day_name\", \"month_name\", \"time_of_day\"]\n",
    "\n",
    "# One-Hot Encoding for nominal categorical variables\n",
    "# This creates new binary columns for each category\n",
    "df_encoded = pd.get_dummies(df_cleaned, columns=categorical_cols_to_encode,\n",
    "                            drop_first=True) # drop_first avoids multicollinearity\n",
    "\n",
    "print(\"\\nOriginal vs Encoded DataFrame shapes:\")\n",
    "print(f\"Original df_cleaned shape: {df_cleaned.shape}\")\n",
    "print(f\"Encoded df_encoded shape: {df_encoded.shape}\")\n",
    "\n",
    "print(\"\\nFirst 5 rows of encoded data (showing new columns):\")\n",
    "print(df_encoded.head())\n",
    "\n",
    "# You can choose to work with df_cleaned (for Power BI) or df_encoded (for ML modeling)\n",
    "# For the purpose of this guide, we will continue with df_cleaned for Power BI export,\n",
    "# but keep df_encoded in mind for the modeling section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd9d6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save enhanced dataset\n",
    "output_enhanced_filename = \"uber_enhanced.csv\"\n",
    "df_cleaned.to_csv(output_enhanced_filename, index=False)\n",
    "\n",
    "print(f\"\\nEnhanced dataset saved as: {output_enhanced_filename}\")\n",
    "print(f\"File size: {len(df_cleaned):,} rows × {len(df_cleaned.columns)} columns\")\n",
    "\n",
    "# Display final enhanced dataset info\n",
    "print(\"\\n=== FINAL ENHANCED DATASET INFO ===\")\n",
    "print(df_cleaned.info())\n",
    "print(\"\\nFirst 5 rows of enhanced data:\")\n",
    "print(df_cleaned.head()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6225edb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== DATASET SPLITTING ===\")\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "# We will use the df_encoded for modeling as it contains one-hot encoded\n",
    "# categorical features\n",
    "# Drop 'key' and 'pickup_datetime' as they are not direct features for modeling\n",
    "# Also drop original categorical columns if using one-hot encoded ones\n",
    "X = df_encoded.drop([\n",
    "    \"fare_amount\", \"key\", \"pickup_datetime\",\n",
    "    \"day_name\", \"month_name\", \"time_of_day\" # Drop original categorical columns\n",
    "], axis=1)\n",
    "y = df_encoded[\"fare_amount\"]\n",
    "\n",
    "print(f\"Features (X) shape: {X.shape}\")\n",
    "print(f\"Target (y) shape: {y.shape}\")\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "# test_size=0.2 means 20% of the data will be used for testing\n",
    "# random_state for reproducibility\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
    "                                                    random_state=42)\n",
    "\n",
    "print(f\"\\nTraining set size: {len(X_train):,} samples\")\n",
    "print(f\"Testing set size: {len(X_test):,} samples\")\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c9a206",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== MODEL SELECTION AND TRAINING ===\")\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "# Initialize the Random Forest Regressor model\n",
    "# n_estimators: number of trees in the forest\n",
    "# random_state: for reproducibility\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1) # n_jobs=-1 uses all available cores\n",
    "\n",
    "print(\"Training the Random Forest Regressor model...\")\n",
    "# Train the model using the training data\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Model training complete.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811ef255",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== MODEL EVALUATION ===\")\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\n--- Regression Model Performance ---\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "print(f\"R-squared (R2): {r2:.4f}\")\n",
    "\n",
    "# Visualize actual vs. predicted values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.3, s=10)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()],\n",
    "         linestyle=\"--\", color=\"red\", linewidth=2, label=\"Perfect Prediction\")\n",
    "plt.title(\"Actual vs. Predicted Fare Amounts\", fontsize=16,\n",
    "          fontweight=\"bold\")\n",
    "plt.xlabel(\"Actual Fare Amount ($)\")\n",
    "plt.ylabel(\"Predicted Fare Amount ($)\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.savefig(\"actual_vs_predicted.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# Visualize residuals\n",
    "residuals = y_test - y_pred\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(residuals, bins=50, kde=True, color=\"skyblue\")\n",
    "plt.title(\"Distribution of Residuals\", fontsize=16, fontweight=\"bold\")\n",
    "plt.xlabel(\"Residuals (Actual - Predicted)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig(\"residuals_distribution.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# Feature Importance (for tree-based models)\n",
    "print(\"\\n--- Feature Importance ---\")\n",
    "feature_importances = pd.Series(model.feature_importances_, index=X.columns)\n",
    "feature_importances = feature_importances.sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x=feature_importances.values, y=feature_importances.index,\n",
    "            palette=\"viridis\")\n",
    "plt.title(\"Feature Importances from Random Forest Regressor\", fontsize=16,\n",
    "          fontweight=\"bold\")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"feature_importance.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Top 10 most important features:\")\n",
    "print(feature_importances.head(10)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
